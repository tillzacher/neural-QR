{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.9.21)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# generate qr code\n",
    "import qrcode\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def generate_qr_code(data, filename=None, version=None, error_correction=qrcode.constants.ERROR_CORRECT_H, border=4, mask_logo=True):\n",
    "    qr = qrcode.QRCode(\n",
    "        version=version,  # 1-40, None\n",
    "        error_correction=error_correction,\n",
    "        box_size=20,  # pixels per box\n",
    "        border=border,  # boxes of border\n",
    "    )\n",
    "    qr.add_data(data)\n",
    "    qr.make(fit=True)\n",
    "    img = qr.make_image()\n",
    "    if filename is not None:\n",
    "        img.save(filename)\n",
    "\n",
    "    if mask_logo:\n",
    "        # add a white square at the center of the QR code, use box_size=20\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "        height, width, _ = img.shape\n",
    "        # add a white square at the center\n",
    "        img[height//2-70:height//2+70, width//2-70:width//2+70] = 255, 255, 255\n",
    "        img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "# function to replace white parts of qr code with noise\n",
    "def add_noise_to_qr_code(qr_code_image, noise_level=0.2, border_noise_level=1.0):\n",
    "    \"\"\"\n",
    "    Adds color noise to a QR code image.\n",
    "\n",
    "    - Replaces the borders around the QR code with color noise proportional to border_noise_level.\n",
    "    - Adds color noise to the black and white sections inside the QR code proportional to noise_level.\n",
    "    - If noise_level or border_noise_level is 0, no change is made to that area.\n",
    "    - If noise_level or border_noise_level is 1, the area is completely replaced with noise.\n",
    "\n",
    "    Parameters:\n",
    "    - qr_code_image: PIL Image object of the QR code.\n",
    "    - noise_level: float (0 to 1), amount of noise to add to the inner QR code.\n",
    "    - border_noise_level: float (0 to 1), amount of noise to add to the border.\n",
    "    \"\"\"\n",
    "    # Convert image to RGB and numpy array (float32 for precision)\n",
    "    qr_array = np.array(qr_code_image.convert('RGB'), dtype=np.float32)\n",
    "\n",
    "    # Get dimensions\n",
    "    height, width, channels = qr_array.shape\n",
    "\n",
    "    # Convert to grayscale for thresholding\n",
    "    qr_gray = np.array(qr_code_image.convert('L'))\n",
    "\n",
    "    # Threshold to create a binary mask (True for white pixels, False for black pixels)\n",
    "    threshold = 128\n",
    "    binary_mask = qr_gray > threshold\n",
    "\n",
    "    # Find the bounding box of the QR code modules (the inner area)\n",
    "    coords = np.column_stack(np.where(binary_mask == False))  # False corresponds to black pixels\n",
    "    if coords.size == 0:\n",
    "        raise ValueError(\"QR code modules not found in the image.\")\n",
    "\n",
    "    top_left = coords.min(axis=0)\n",
    "    bottom_right = coords.max(axis=0)\n",
    "\n",
    "    # Create masks for border and inner areas\n",
    "    border_mask = np.ones((height, width), dtype=bool)\n",
    "    inner_mask = np.zeros((height, width), dtype=bool)\n",
    "\n",
    "    # Define the inner QR code area\n",
    "    inner_mask[top_left[0]:bottom_right[0]+1, top_left[1]:bottom_right[1]+1] = True\n",
    "    border_mask[inner_mask] = False  # Border is where inner_mask is False\n",
    "\n",
    "    # Apply noise to border area\n",
    "    if border_noise_level > 0:\n",
    "        # Generate color noise for border area\n",
    "        border_noise = np.random.randint(0, 256, size=(height, width, channels), dtype=np.uint8)\n",
    "        border_noise = border_noise.astype(np.float32)\n",
    "        # Blend the original border area with noise based on border_noise_level\n",
    "        qr_array[border_mask] = (1 - border_noise_level) * qr_array[border_mask] + border_noise_level * border_noise[border_mask]\n",
    "    # Else, border_noise_level == 0, so no change to border area\n",
    "\n",
    "    # Apply noise to inner area\n",
    "    if noise_level > 0:\n",
    "        # Generate color noise for inner area\n",
    "        inner_noise = np.random.randint(0, 256, size=(height, width, channels), dtype=np.uint8)\n",
    "        inner_noise = inner_noise.astype(np.float32)\n",
    "        # Blend the original inner area with noise based on noise_level\n",
    "        qr_array[inner_mask] = (1 - noise_level) * qr_array[inner_mask] + noise_level * inner_noise[inner_mask]\n",
    "    # Else, noise_level == 0, so no change to inner area\n",
    "\n",
    "    # Convert back to uint8\n",
    "    qr_array = np.clip(qr_array, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert back to PIL Image\n",
    "    noisy_qr_code = Image.fromarray(qr_array, 'RGB')\n",
    "\n",
    "    return noisy_qr_code\n",
    "\n",
    "# Prompt Generator\n",
    "import random\n",
    "\n",
    "# Template: \"A QR code {verb} into a {adjective} {location}, {artstyle}\"\n",
    "\n",
    "def generate_prompt():\n",
    "    verbs = [\n",
    "        'integrated',\n",
    "        'blended',\n",
    "        'embedded',\n",
    "        'merged',\n",
    "        'incorporated',\n",
    "        'interwoven',\n",
    "        'woven',\n",
    "        'fused',\n",
    "        'engraved',\n",
    "        'imprinted'\n",
    "    ]\n",
    "    \n",
    "    adjectives = [\n",
    "        'futuristic',\n",
    "        'ancient',\n",
    "        'mystical',\n",
    "        'vibrant',\n",
    "        'serene',\n",
    "        'abstract',\n",
    "        'surreal',\n",
    "        'majestic',\n",
    "        'ethereal',\n",
    "        'dynamic',\n",
    "        'colorful',\n",
    "        'monochrome',\n",
    "        'minimalist',\n",
    "        'intricate',\n",
    "        'ornate'\n",
    "    ]\n",
    "    \n",
    "    locations = [\n",
    "        'cityscape',\n",
    "        'forest',\n",
    "        'mountain landscape',\n",
    "        'underwater scene',\n",
    "        'space nebula',\n",
    "        'desert',\n",
    "        'ocean',\n",
    "        'galaxy',\n",
    "        'garden',\n",
    "        'skyline',\n",
    "        'countryside',\n",
    "        'rainforest',\n",
    "        'ice cave',\n",
    "        'ancient temple',\n",
    "        'futuristic metropolis'\n",
    "    ]\n",
    "    \n",
    "    artstyles = [\n",
    "        'digital art',\n",
    "        'oil painting',\n",
    "        'watercolor',\n",
    "        'pencil sketch',\n",
    "        'cyberpunk style',\n",
    "        'steampunk aesthetic',\n",
    "        'fantasy art',\n",
    "        'minimalist design',\n",
    "        'photorealistic rendering',\n",
    "        'pop art',\n",
    "        'impressionist painting',\n",
    "        'surrealism',\n",
    "        'abstract expressionism',\n",
    "        'low-poly art',\n",
    "        'graffiti style'\n",
    "    ]\n",
    "\n",
    "    #verb = random.choice(verbs)\n",
    "    verb = 'integrated'\n",
    "    adjective = random.choice(adjectives)\n",
    "    location = random.choice(locations)\n",
    "    artstyle = random.choice(artstyles)\n",
    "\n",
    "    prompt = f\"A QR code, cleverly {verb} into a {adjective} {location}, {artstyle}, 3d rendered\"\n",
    "    # Generate filename that includes information from prompt but short and unique\n",
    "    filename = f\"{adjective}_{location}_{artstyle}\"\n",
    "    return prompt, filename\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, DDIMScheduler\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "#from realesrgan import RealESRGAN\n",
    "\n",
    "def resize_for_condition_image(input_image: Image.Image, resolution: int):\n",
    "    input_image = input_image.convert(\"RGB\")\n",
    "    W, H = input_image.size\n",
    "    k = float(resolution) / min(H, W)\n",
    "    H = int(H * k)\n",
    "    W = int(W * k)\n",
    "    H = (H // 64) * 64\n",
    "    W = (W // 64) * 64\n",
    "    img = input_image.resize((W, H), resample=Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "from PIL import ImageOps\n",
    "\n",
    "def run_diffusion_on_qr_code(\n",
    "    image_of_qr,\n",
    "    prompt,\n",
    "    output_filename,\n",
    "    model_id='dreamlike-art/dreamlike-diffusion-1.0',\n",
    "    controlnet_model_id='DionTimmer/controlnet_qrcode-control_v1p_sd15',\n",
    "    strength=0.9,\n",
    "    guidance_scale=20,\n",
    "    controlnet_conditioning_scale=1.5,\n",
    "    num_inference_steps=150,\n",
    "    resolution=768,\n",
    "    seed=11111,\n",
    "    noise_level=0.5,\n",
    "    border_noise_level=1.0,\n",
    "    verbose=False,\n",
    "    invert_colors=False,\n",
    "    upscale_resolution=None\n",
    "):\n",
    "    \n",
    "    output_filename = f'output_images/{output_filename}'\n",
    "    if verbose:\n",
    "        print(f'Running qr code diffusion with prompt: {prompt}')\n",
    "    # Check for available devices\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch_dtype = torch.float16  # CUDA supports float16 for performance\n",
    "        if verbose:\n",
    "            print(\"Using CUDA device\")\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "        torch_dtype = torch.float32  # MPS supports float32\n",
    "        if verbose:\n",
    "            print(\"Using MPS device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        torch_dtype = torch.float32  # CPU uses float32\n",
    "        if verbose:\n",
    "            print(\"Using CPU device\")\n",
    "\n",
    "    # Load ControlNet model\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        controlnet_model_id,\n",
    "        torch_dtype=torch_dtype\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the pipeline\n",
    "    pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "        'models/animerge_v23',\n",
    "        controlnet=controlnet,\n",
    "        safety_checker=None,\n",
    "        torch_dtype=torch_dtype\n",
    "    ).to(device)\n",
    "\n",
    "    # Memory optimization\n",
    "    if device.type == 'cuda':\n",
    "        # Disable xFormers due to incompatibility\n",
    "        if verbose:\n",
    "            print(\"xFormers not compatible with this GPU; enabling attention slicing instead.\")\n",
    "        pipe.enable_attention_slicing()\n",
    "    elif device.type == 'mps':\n",
    "        pipe.enable_attention_slicing()\n",
    "    else:\n",
    "        pipe.enable_attention_slicing()\n",
    "\n",
    "    # Optional: Enable other memory optimizations\n",
    "    pipe.enable_model_cpu_offload()\n",
    "\n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    # Resize the QR code image (condition image)\n",
    "    condition_image = resize_for_condition_image(image_of_qr, resolution)\n",
    "\n",
    "\n",
    "    # Prepare the initial image\n",
    "    init_image = image_of_qr.copy()\n",
    "    init_image = add_noise_to_qr_code(init_image, noise_level, border_noise_level)\n",
    "    init_image = resize_for_condition_image(init_image, resolution)\n",
    "\n",
    "    if invert_colors:\n",
    "        init_image = init_image.convert(\"RGB\")\n",
    "        init_image = ImageOps.invert(init_image)\n",
    "\n",
    "    # Set a random seed for reproducibility\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    # Generate the image\n",
    "    result = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=\"ugly, disfigured, low quality, blurry\",\n",
    "        image=init_image,\n",
    "        control_image=condition_image,\n",
    "        width=condition_image.width,\n",
    "        height=condition_image.height,\n",
    "        guidance_scale=guidance_scale,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        generator=generator,\n",
    "        strength=strength,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "    )\n",
    "\n",
    "    final_image = result.images[0]\n",
    "\n",
    "    # Upscaling with RealESRGAN\n",
    "    if upscale_resolution is not None:\n",
    "        if verbose:\n",
    "            print(f\"Upscaling image to resolution: {upscale_resolution}\")\n",
    "\n",
    "        model = RealESRGAN(device, scale=4)\n",
    "        model.load_weights('RealESRGAN_x4.pth')  # Make sure to download weights\n",
    "\n",
    "        # Calculate scaling factor to reach desired upscale resolution\n",
    "        scale_factor = max(upscale_resolution / final_image.width, upscale_resolution / final_image.height)\n",
    "        intermediate_size = (int(final_image.width * scale_factor), int(final_image.height * scale_factor))\n",
    "\n",
    "        # Perform AI upscaling\n",
    "        final_image = model.predict(final_image.resize(intermediate_size, resample=Image.LANCZOS))\n",
    "\n",
    "        # Crop or resize exactly to upscale_resolution if necessary\n",
    "        final_image = final_image.resize((upscale_resolution, upscale_resolution), Image.LANCZOS)\n",
    "\n",
    "    final_image.save(output_filename)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Image saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QR code with message: https://www.linkedin.com/in/till-zacher/\n",
      "Generated Prompt: A QR code, cleverly integrated into a minimalist garden, cyberpunk style, 3d rendered\n"
     ]
    }
   ],
   "source": [
    "import qrcode\n",
    "\n",
    "message = 'https://www.linkedin.com/in/till-zacher/'\n",
    "qr_filename = 'LinkedIn.png' # qr will not be saved to disk\n",
    "image_of_qr = generate_qr_code(message, filename=qr_filename, error_correction=qrcode.constants.ERROR_CORRECT_H, border=10, mask_logo=True)\n",
    "print(f\"Generated QR code with message: {message}\")\n",
    "# show image in jupyter notebook\n",
    "#image_of_qr.show()\n",
    "\n",
    "# Generate the prompt\n",
    "prompt, prompt_filename = generate_prompt()\n",
    "print(f\"Generated Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed 384\n",
      "Running qr code diffusion with prompt: secret chamber inside an ice cave\n",
      "Using CUDA device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c249748d5cc94e8788edbd2efdcfdc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch models/animerge_v23\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch models/animerge_v23\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet_img2img.StableDiffusionControlNetImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xFormers not compatible with this GPU; enabling attention slicing instead.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#output_filename = f'{message}_{prompt_filename}_s{strength}_gs{guidance_scale}_ccs{controlnet_conditioning_scale}.png'\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#output_filename = f'{message}_{prompt_filename}.png'\u001b[39;00m\n\u001b[0;32m     27\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mrun_diffusion_on_qr_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_of_qr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_of_qr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mborder_noise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mborder_noise_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minvert_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert_colors\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 269\u001b[0m, in \u001b[0;36mrun_diffusion_on_qr_code\u001b[1;34m(image_of_qr, prompt, output_filename, model_id, controlnet_model_id, strength, guidance_scale, controlnet_conditioning_scale, num_inference_steps, resolution, seed, noise_level, border_noise_level, verbose, invert_colors, upscale_resolution)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# Optional: Enable other memory optimizations\u001b[39;00m\n\u001b[0;32m    267\u001b[0m pipe\u001b[38;5;241m.\u001b[39menable_model_cpu_offload()\n\u001b[1;32m--> 269\u001b[0m pipe\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[43mDDIMScheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# Resize the QR code image (condition image)\u001b[39;00m\n\u001b[0;32m    272\u001b[0m condition_image \u001b[38;5;241m=\u001b[39m resize_for_condition_image(image_of_qr, resolution)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\diffusion\\lib\\site-packages\\diffusers\\configuration_utils.py:263\u001b[0m, in \u001b[0;36mConfigMixin.from_config\u001b[1;34m(cls, config, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m         init_dict[deprecated_kwarg] \u001b[38;5;241m=\u001b[39m unused_kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_kwarg)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Return model and optionally state and/or unused_kwargs\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_dict)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# make sure to also save config parameters that might be used for compatible classes\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# update _class_name\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hidden_dict:\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\diffusion\\lib\\site-packages\\diffusers\\configuration_utils.py:693\u001b[0m, in \u001b[0;36mregister_to_config.<locals>.inner_init\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_init_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs}\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_to_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n\u001b[1;32m--> 693\u001b[0m init(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\diffusion\\lib\\site-packages\\diffusers\\schedulers\\scheduling_ddim.py:234\u001b[0m, in \u001b[0;36mDDIMScheduler.__init__\u001b[1;34m(self, num_train_timesteps, beta_start, beta_end, beta_schedule, trained_betas, clip_sample, set_alpha_to_one, steps_offset, prediction_type, thresholding, dynamic_thresholding_ratio, clip_sample_range, sample_max_value, timestep_spacing, rescale_betas_zero_snr)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# setable values\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_inference_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train_timesteps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run once\n",
    "import random\n",
    "strength = 0.9\n",
    "guidance_scale = 7  # how much to focus on the prompt\n",
    "controlnet_conditioning_scale = 1.2 # how much to focus on the condition image\n",
    "resolution = 1024\n",
    "seed = 384\n",
    "#seed = random.randint(0, 100000)\n",
    "print(f\"Using seed {seed}\")\n",
    "num_steps = 100 # 30\n",
    "noise_level = 0.6\n",
    "border_noise_level = 0.3\n",
    "verbose = True\n",
    "invert_colors = True\n",
    "\n",
    "#prompt_override = 'magical computer hardware, electronics, futuristic, abstract'\n",
    "prompt_override = 'secret chamber inside an ice cave'\n",
    "#\n",
    "# prompt_override = None\n",
    "if prompt_override is not None:\n",
    "    prompt = prompt_override\n",
    "else:\n",
    "    prompt, prompt_filename = generate_prompt()\n",
    "\n",
    "#output_filename = f'{message}_{prompt_filename}_s{strength}_gs{guidance_scale}_ccs{controlnet_conditioning_scale}.png'\n",
    "#output_filename = f'{message}_{prompt_filename}.png'\n",
    "output_filename = 'Test.png'\n",
    "\n",
    "run_diffusion_on_qr_code(image_of_qr=image_of_qr, prompt=prompt, strength=strength, guidance_scale=guidance_scale, controlnet_conditioning_scale=controlnet_conditioning_scale, resolution=resolution, seed=seed, output_filename=output_filename, num_inference_steps=num_steps, noise_level=noise_level, verbose=verbose, border_noise_level=border_noise_level, invert_colors=invert_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations to process: 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90351ebdbf44b529439472a5e8ea7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch models/animerge_v23\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch models/animerge_v23\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet_img2img.StableDiffusionControlNetImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 56\u001b[0m\n\u001b[0;32m     50\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     51\u001b[0m     output_dir,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_s\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mguidance_scale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ccs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontrolnet_conditioning_scale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_nl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoise_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_bnl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborder_noise_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_steps\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_inv\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvert_colors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Run the diffusion process\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[43mrun_diffusion_on_qr_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_of_qr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_of_qr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mborder_noise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mborder_noise_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvert_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert_colors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 269\u001b[0m, in \u001b[0;36mrun_diffusion_on_qr_code\u001b[1;34m(image_of_qr, prompt, output_filename, model_id, controlnet_model_id, strength, guidance_scale, controlnet_conditioning_scale, num_inference_steps, resolution, seed, noise_level, border_noise_level, verbose, invert_colors, upscale_resolution)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# Optional: Enable other memory optimizations\u001b[39;00m\n\u001b[0;32m    267\u001b[0m pipe\u001b[38;5;241m.\u001b[39menable_model_cpu_offload()\n\u001b[1;32m--> 269\u001b[0m pipe\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[43mDDIMScheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# Resize the QR code image (condition image)\u001b[39;00m\n\u001b[0;32m    272\u001b[0m condition_image \u001b[38;5;241m=\u001b[39m resize_for_condition_image(image_of_qr, resolution)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\diffusion\\lib\\site-packages\\diffusers\\configuration_utils.py:260\u001b[0m, in \u001b[0;36mConfigMixin.from_config\u001b[1;34m(cls, config, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m         init_dict[deprecated_kwarg] \u001b[38;5;241m=\u001b[39m unused_kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_kwarg)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Return model and optionally state and/or unused_kwargs\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_dict)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# make sure to also save config parameters that might be used for compatible classes\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# update _class_name\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hidden_dict:\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\diffusion\\lib\\site-packages\\diffusers\\configuration_utils.py:665\u001b[0m, in \u001b[0;36mregister_to_config.<locals>.inner_init\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_init_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs}\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_to_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n\u001b[1;32m--> 665\u001b[0m init(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\diffusion\\lib\\site-packages\\diffusers\\schedulers\\scheduling_ddim.py:234\u001b[0m, in \u001b[0;36mDDIMScheduler.__init__\u001b[1;34m(self, num_train_timesteps, beta_start, beta_end, beta_schedule, trained_betas, clip_sample, set_alpha_to_one, steps_offset, prediction_type, thresholding, dynamic_thresholding_ratio, clip_sample_range, sample_max_value, timestep_spacing, rescale_betas_zero_snr)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# setable values\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_inference_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train_timesteps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Define the ranges for each parameter\n",
    "strength_values = [0.9]  # Example values for strength\n",
    "guidance_scale_values = [7, 15]  # Example values for guidance_scale\n",
    "controlnet_conditioning_scale_values = [1.9, 2.6]  # Example values for controlnet_conditioning_scale\n",
    "resolution_values = [1024]  # Example resolution\n",
    "# generate integer seed range\n",
    "seed_values = [random.randint(0, 100000) for i in range(8)]  # Example values for seed\n",
    "noise_level_values = [0.3, 0.6]  # Example values for noise_level\n",
    "border_noise_level_values = [0.3, 0.6]\n",
    "num_inference_steps_values = [30]  # Example values for num_inference_steps\n",
    "invert_colors_values = [True]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"data_science_test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "parameter_combinations = list(itertools.product(\n",
    "    strength_values,\n",
    "    guidance_scale_values,\n",
    "    controlnet_conditioning_scale_values,\n",
    "    resolution_values,\n",
    "    seed_values,\n",
    "    noise_level_values,\n",
    "    border_noise_level_values,\n",
    "    num_inference_steps_values,\n",
    "    invert_colors_values\n",
    "))\n",
    "\n",
    "print(f\"Total combinations to process: {len(parameter_combinations)}\")\n",
    "\n",
    "# Loop over each combination and run the diffusion process\n",
    "for idx, (strength, guidance_scale, controlnet_conditioning_scale, resolution, seed, noise_level, border_noise_level, num_steps, invert_colors) in enumerate(parameter_combinations):\n",
    "\n",
    "    # If using a prompt override, ensure it's set\n",
    "    #prompt_override = 'A busy chinese market scene, has a QR code cleverly embedded into the scene, 3D, animated'\n",
    "    prompt_override = 'magical computer hardware, electronics, futuristic, abstract'\n",
    "    if prompt_override is not None:\n",
    "        prompt = prompt_override\n",
    "        prompt_filename = \"prompt_override\"\n",
    "    else:\n",
    "        # Generate the prompt\n",
    "        prompt, prompt_filename = generate_prompt()\n",
    "\n",
    "    # Generate a unique output filename for each combination\n",
    "    output_filename = os.path.join(\n",
    "        output_dir,\n",
    "        f\"{message}_{prompt_filename}_s{strength}_gs{guidance_scale}_ccs{controlnet_conditioning_scale}_r{resolution}_seed{seed}_nl{noise_level}_bnl{border_noise_level}_steps{num_steps}_inv{invert_colors}.png\"\n",
    "    )\n",
    "\n",
    "    # Run the diffusion process\n",
    "    run_diffusion_on_qr_code(\n",
    "        image_of_qr=image_of_qr,\n",
    "        prompt=prompt,\n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        resolution=resolution,\n",
    "        seed=seed,\n",
    "        output_filename=output_filename,\n",
    "        num_inference_steps=num_steps,\n",
    "        noise_level=noise_level,\n",
    "        border_noise_level=border_noise_level,\n",
    "        invert_colors=invert_colors,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
