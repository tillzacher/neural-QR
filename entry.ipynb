{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40507ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A QR code, cleverly embedded into a monochrome futuristic metropolis, metallic theme, 3D render, 3D rendered\n",
      "Filename: monochrome_futuristic_metropolis_3D_render_metallic\n"
     ]
    }
   ],
   "source": [
    "from modules.diffuser import run_diffusion_on_qr_code\n",
    "from modules.input_gen import generate_prompt\n",
    "from modules.input_gen import generate_wifi_qr_string\n",
    "\n",
    "#message = generate_wifi_qr_string('SSID', 'PASSWORD')\n",
    "message = 'https://github.com/tillzacher/qr_code'\n",
    "#message = 'Hello World!'\n",
    "prompt, filename = generate_prompt()\n",
    "prompt_override = 'prompt here'\n",
    "prompt_override = None\n",
    "if prompt_override:\n",
    "    prompt = prompt_override\n",
    "    filename = prompt.replace(' ', '_')\n",
    "    filename = filename.replace(',', '')\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Filename: {filename}')\n",
    "border = 10\n",
    "mask_logo = 6\n",
    "\n",
    "center_noise_level = 0.9\n",
    "noise_level = 0.5\n",
    "border_noise_level = 0.9\n",
    "\n",
    "model_id = 'models/animerge_v23'\n",
    "resolution = 1152\n",
    "seed = 384\n",
    "\n",
    "invert_colors = False\n",
    "guidance_scale = 9 # adhere to the prompt\n",
    "controlnet_conditioning_scale=1.1 # adhere to the qr code\n",
    "strength=0.9\n",
    "num_inference_steps=50\n",
    "verbose=False\n",
    "continuous=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ede84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]An error occurred while trying to fetch models/animerge_v23\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  50%|█████     | 3/6 [00:00<00:00,  7.28it/s]An error occurred while trying to fetch models/animerge_v23\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:01<00:00,  4.59it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet_img2img.StableDiffusionControlNetImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "It seems like you have activated model offloading by calling `enable_model_cpu_offload`, but are now manually moving the pipeline to GPU. It is strongly recommended against doing so as memory gains from offloading are likely to be lost. Offloading automatically takes care of moving the individual components vae, text_encoder, tokenizer, unet, controlnet, scheduler, safety_checker, feature_extractor, image_encoder to GPU when needed. To make sure offloading works as expected, you should consider moving the pipeline back to CPU: `pipeline.to('cpu')` or removing the move altogether if you use offloading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt: A QR code, cleverly embedded into a monochrome futuristic metropolis, metallic theme, 3D render, 3D rendered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "run_diffusion_on_qr_code(\n",
    "    message,\n",
    "    prompt,\n",
    "    filename,\n",
    "    border=border,\n",
    "    mask_logo=mask_logo,\n",
    "    center_noise_level=center_noise_level,\n",
    "    noise_level=noise_level,\n",
    "    border_noise_level=border_noise_level,\n",
    "    model_id=model_id,\n",
    "    resolution=resolution,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    seed=seed,\n",
    "    invert_colors=invert_colors,\n",
    "    guidance_scale=guidance_scale,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    strength=strength,\n",
    "    verbose=verbose,\n",
    "    continuous=continuous\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9ae1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A QR code, cleverly merged into a minimalist cyberpunk alley, transparent theme, digital art, 3D rendered\n",
      "Filename: minimalist_cyberpunk_alley_digital_art_transparent\n",
      "Running with parameters: noise_level=0.0, center_noise_level=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]An error occurred while trying to fetch models/animerge_v23\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  50%|█████     | 3/6 [00:00<00:00,  6.51it/s]An error occurred while trying to fetch models/animerge_v23\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  83%|████████▎ | 5/6 [00:01<00:00,  3.03it/s]c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:01<00:00,  4.15it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet_img2img.StableDiffusionControlNetImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "It seems like you have activated model offloading by calling `enable_model_cpu_offload`, but are now manually moving the pipeline to GPU. It is strongly recommended against doing so as memory gains from offloading are likely to be lost. Offloading automatically takes care of moving the individual components vae, text_encoder, tokenizer, unet, controlnet, scheduler, safety_checker, feature_extractor, image_encoder to GPU when needed. To make sure offloading works as expected, you should consider moving the pipeline back to CPU: `pipeline.to('cpu')` or removing the move altogether if you use offloading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt: A QR code, cleverly merged into a minimalist cyberpunk alley, transparent theme, digital art, 3D rendered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\diffusers\\pipelines\\controlnet\\pipeline_controlnet_img2img.py:1047: FutureWarning: `callback` is deprecated and will be removed in version 1.0.0. Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\n",
      "  deprecate(\n",
      "c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\diffusers\\pipelines\\controlnet\\pipeline_controlnet_img2img.py:1053: FutureWarning: `callback_steps` is deprecated and will be removed in version 1.0.0. Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`\n",
      "  deprecate(\n",
      "  6%|▌         | 1/18 [00:00<00:13,  1.28it/s]c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\diffusers\\pipelines\\controlnet\\pipeline_controlnet_img2img.py:547: FutureWarning: The decode_latents method is deprecated and will be removed in 1.0.0. Please use VaeImageProcessor.postprocess(...) instead\n",
      "  deprecate(\"decode_latents\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "100%|██████████| 18/18 [00:23<00:00,  1.33s/it]\n",
      "Exception in thread Thread-13 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\subprocess.py\", line 1515, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x81 in position 130: invalid start byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with parameters: noise_level=0.0, center_noise_level=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]An error occurred while trying to fetch models/animerge_v23\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  50%|█████     | 3/6 [00:00<00:00,  7.13it/s]An error occurred while trying to fetch models/animerge_v23\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory models/animerge_v23\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:01<00:00,  4.22it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet_img2img.StableDiffusionControlNetImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "It seems like you have activated model offloading by calling `enable_model_cpu_offload`, but are now manually moving the pipeline to GPU. It is strongly recommended against doing so as memory gains from offloading are likely to be lost. Offloading automatically takes care of moving the individual components vae, text_encoder, tokenizer, unet, controlnet, scheduler, safety_checker, feature_extractor, image_encoder to GPU when needed. To make sure offloading works as expected, you should consider moving the pipeline back to CPU: `pipeline.to('cpu')` or removing the move altogether if you use offloading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt: A QR code, cleverly merged into a minimalist cyberpunk alley, transparent theme, digital art, 3D rendered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [00:08<00:13,  1.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m sweep_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoise_level\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m],\n\u001b[0;32m     37\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter_noise_level\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]}\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Now run the sweep.\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43mrun_parameter_sweep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43msweep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# base filename that will be appended with suffixes.\u001b[39;49;00m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mborder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mborder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_logo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_logo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_noise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_noise_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mborder_noise_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mborder_noise_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_model_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDionTimmer/controlnet_qrcode-control_v1p_sd15\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvert_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert_colors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     60\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\Documents\\qr_code\\modules\\diffuser.py:296\u001b[0m, in \u001b[0;36mrun_parameter_sweep\u001b[1;34m(sweep_params, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m if \"filename\" in current_kwargs:\n\u001b[0;32m    295\u001b[0m     current_kwargs[\"filename\"] = current_kwargs[\"filename\"] + suffix\n\u001b[1;32m--> 296\u001b[0m else:\n\u001b[0;32m    297\u001b[0m     current_kwargs[\"filename\"] = \"output\" + suffix\n\u001b[0;32m    299\u001b[0m print(\"Running with parameters:\", \", \".join(f\"{k}={v}\" for k, v in zip(keys, combination)))\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\Documents\\qr_code\\modules\\diffuser.py:251\u001b[0m, in \u001b[0;36mrun_diffusion_on_qr_code\u001b[1;34m(message, prompt, filename, border, mask_logo, center_noise_level, noise_level, border_noise_level, model_id, controlnet_model_id, resolution, seed, invert_colors, guidance_scale, controlnet_conditioning_scale, strength, num_inference_steps, verbose, continuous)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobocopy reported an error. Return code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, proc\u001b[38;5;241m.\u001b[39mreturncode)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout:\u001b[39m\u001b[38;5;124m\"\u001b[39m, proc\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstderr:\u001b[39m\u001b[38;5;124m\"\u001b[39m, proc\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\Documents\\qr_code\\modules\\diffuser.py:210\u001b[0m, in \u001b[0;36mrun_diffusion_on_qr_code.<locals>.generate_and_save\u001b[1;34m()\u001b[0m\n\u001b[0;32m    208\u001b[0m out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_anim_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatest_intermediate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m pil_img\u001b[38;5;241m.\u001b[39msave(out_path)\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntermediate image saved at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in folder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_anim_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\diffusers\\pipelines\\controlnet\\pipeline_controlnet_img2img.py:1304\u001b[0m, in \u001b[0;36mStableDiffusionControlNetImg2ImgPipeline.__call__\u001b[1;34m(self, prompt, image, control_image, height, width, strength, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m callback_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1303\u001b[0m         step_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1304\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m XLA_AVAILABLE:\n\u001b[0;32m   1307\u001b[0m     xm\u001b[38;5;241m.\u001b[39mmark_step()\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\Documents\\qr_code\\modules\\diffuser.py:197\u001b[0m, in \u001b[0;36mrun_diffusion_on_qr_code.<locals>.generate_and_save.<locals>.save_intermediate\u001b[1;34m(step, timestep, latents)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_inference_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# save only every 2nd step\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\diffusers\\pipelines\\controlnet\\pipeline_controlnet_img2img.py:550\u001b[0m, in \u001b[0;36mStableDiffusionControlNetImg2ImgPipeline.decode_latents\u001b[1;34m(self, latents)\u001b[0m\n\u001b[0;32m    547\u001b[0m deprecate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode_latents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, deprecation_message, standard_warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    549\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mscaling_factor \u001b[38;5;241m*\u001b[39m latents\n\u001b[1;32m--> 550\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    551\u001b[0m image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\diffusers\\utils\\accelerate_utils.py:45\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\accelerate\\hooks.py:720\u001b[0m, in \u001b[0;36mCpuOffload.pre_forward\u001b[1;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_module_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_module_hook\u001b[38;5;241m.\u001b[39moffload()\n\u001b[1;32m--> 720\u001b[0m     \u001b[43mclear_device_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m module\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device)\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device)\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\accelerate\\utils\\memory.py:64\u001b[0m, in \u001b[0;36mclear_device_cache\u001b[1;34m(garbage_collection)\u001b[0m\n\u001b[0;32m     62\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_cuda_available():\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_hpu_available():\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# torch.hpu.empty_cache() # not available on hpu as it reserves all device memory for the current process\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Schweini\\miniconda3\\envs\\torch_diffusers_env\\lib\\site-packages\\torch\\cuda\\memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from modules.diffuser import run_diffusion_on_qr_code, run_parameter_sweep\n",
    "from modules.input_gen import generate_prompt, generate_wifi_qr_string\n",
    "\n",
    "# Generate your message and prompt as before.\n",
    "message = generate_wifi_qr_string('SSID', 'Password')\n",
    "message = 'https://github.com/tillzacher/qr_code'\n",
    "prompt, filename = generate_prompt()\n",
    "prompt_override = None\n",
    "if prompt_override:\n",
    "    prompt = prompt_override\n",
    "    filename = prompt.replace(' ', '_')\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Filename: {filename}')\n",
    "\n",
    "# Constant parameters:\n",
    "border = 10\n",
    "mask_logo = 4\n",
    "\n",
    "center_noise_level = 0.6\n",
    "noise_level = 0.2\n",
    "border_noise_level = 0.6\n",
    "\n",
    "model_id = 'models/animerge_v23'\n",
    "resolution = 768\n",
    "seed = 384\n",
    "\n",
    "invert_colors = True\n",
    "guidance_scale = 6\n",
    "controlnet_conditioning_scale = 1.0\n",
    "strength = 0.9\n",
    "num_inference_steps = 20\n",
    "verbose = False\n",
    "\n",
    "# Sweep over values: eg {\"guidance_scale\": [1, 5, 10, 15, 20]}\n",
    "sweep_params = {\"noise_level\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                \"center_noise_level\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]}\n",
    "\n",
    "# Now run the sweep.\n",
    "run_parameter_sweep(\n",
    "    sweep_params,\n",
    "    message=message,\n",
    "    prompt=prompt,\n",
    "    filename=filename,  # base filename that will be appended with suffixes.\n",
    "    border=border,\n",
    "    mask_logo=mask_logo,\n",
    "    center_noise_level=center_noise_level,\n",
    "    noise_level=noise_level,\n",
    "    border_noise_level=border_noise_level,\n",
    "    model_id=model_id,\n",
    "    controlnet_model_id='DionTimmer/controlnet_qrcode-control_v1p_sd15',\n",
    "    resolution=resolution,\n",
    "    seed=seed,\n",
    "    invert_colors=invert_colors,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    strength=strength,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    verbose=verbose,\n",
    "    continuous=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea76eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_diffusers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
